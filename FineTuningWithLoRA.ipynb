{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFGBwfQWYP1P"
   },
   "source": [
    "# Fine-Tuning LLaMA2 for News-Aware Conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUMhIm0ZYjP3"
   },
   "source": [
    "This activity explores advanced techniques for fine-tuning LLaMA-2 using Parameter-Efficient Fine-Tuning (PEFT) with the Low-Rank Adaptation (LoRA) method.\n",
    "\n",
    "We will work with a customized version of the AGNews dataset, reformatted to match the instruction format of the Alpaca dataset. This ensures that our model is trained on a structured input format, optimized for instruction-based fine-tuning in text classification tasks.\n",
    "\n",
    "Through supervised instruction fine-tuning, we will train our model to perform tasks specified as natural language instructions. These instructions are centered toward accurately classifying input text into one of the following categories: Business, World, Sci/Tech, and Sports.\n",
    "\n",
    "After fine-tuning, we will evaluate the model’s conversational performance using automated benchmarks derived from another state-of-the-art language model, DeepSeek-R1-Distill-Llama-8B.\n",
    "\n",
    "These benchmarks will measure the model’s ability to engage in meaningful and context-aware conversations, evaluating key aspects such as:\n",
    "\n",
    "* Coherence in responses\n",
    "* Relevance to user input\n",
    "* Understanding of nuanced conversational cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpjoLp44Zuu0"
   },
   "source": [
    "## Setting up the enviroment\n",
    "\n",
    "Installs the dependent libraries for efficient training, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 99782,
     "status": "ok",
     "timestamp": 1740718975340,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "RJqCdxJ21n4V",
    "outputId": "65f85710-384a-475f-b67b-cbd1e098648f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 150756,
     "status": "ok",
     "timestamp": 1740719130955,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "e356gtyhZoOv",
    "outputId": "fae2f95a-3f07-4b46-f55d-ec8cd5a69b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Collecting torchao\n",
      "  Downloading torchao-0.8.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.8.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchao-0.8.0 xxhash-3.5.0\n",
      "Collecting torchtune\n",
      "  Downloading torchtune-0.5.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.2)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.9)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
      "Collecting tiktoken (from torchtune)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting blobfile>=2 (from torchtune)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
      "Collecting omegaconf (from torchtune)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune)\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (2.3.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->torchtune)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Downloading torchtune-0.5.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.3/810.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=3aa91b21ebb7b522903a6a0abd8fb2cf340ecad9b9753c7973fdbd530397c641\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, pycryptodomex, omegaconf, hf-transfer, tiktoken, blobfile, torchtune\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 blobfile-3.0.0 hf-transfer-0.1.9 omegaconf-2.3.0 pycryptodomex-3.21.0 tiktoken-0.9.0 torchtune-0.5.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "12cca1132ac0483083753468bd4d0860",
       "pip_warning": {
        "packages": [
         "pydevd_plugins"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchao transformers datasets wandb peft evaluate scikit-learn\n",
    "!pip install torchtune # For Fine Tuning\n",
    "!pip install ollama    # For evaluation with DeepSeek-R1\n",
    "!pip install wandb     # For Logging and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov00p7N_bUyY"
   },
   "source": [
    "## Get Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c61ad0975df24ee29464a1cbc23284aa",
      "84b0c8e5b88c42d69eead53736f12c9e",
      "0f17447703644549818025c2036a8ec3",
      "5054142b3b01464e8c896aad70cc8e99",
      "47bdc671486743fbaec75be63e5a0fc5",
      "ad8bbe2c1aee46b8bc56503252b05740",
      "a96cf73088ba4b5fbe340711a463c4bb",
      "3da7bd85ef8a4ca19da51c792e5bb4ac",
      "6a1892fbd1c64e6c9d7bc4d64d88dbf3",
      "82322fade0044806a93a4898057b9526",
      "e367d720ee9947cf89863d95e2fa3643"
     ]
    },
    "executionInfo": {
     "elapsed": 245481,
     "status": "ok",
     "timestamp": 1740671307993,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "v4a_awIZbpD0",
    "outputId": "ade8f223-1bca-41e2-8b6a-704656df59e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.model from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
      "loading file tokenizer.json from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/pytorch_model.bin.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61ad0975df24ee29464a1cbc23284aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is cached at: meta-llama/Llama-2-7b-hf\n",
      "Tokenizer is cached at: meta-llama/Llama-2-7b-hf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "os.environ[\"HF_TOKEN\"] = \"<blah>\"\n",
    "login(token=os.environ.get(\"HF_TOKEN\"))\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.get_logger(\"transformers\").setLevel(logging.INFO)\n",
    "\n",
    "# Download the baseline LLaMA-2 model https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache\", use_safetensors=False)\n",
    "\n",
    "print(f\"Model is cached at: {model.name_or_path}\")\n",
    "print(f\"Tokenizer is cached at: {tokenizer.name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Udo0pKSzj2ID"
   },
   "source": [
    "## Load AGNews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1740621119122,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "5GsGAtPiZvXr",
    "outputId": "09de0b1f-0dbe-461c-8e52-4bf671b53026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9710,
     "status": "ok",
     "timestamp": 1740621169233,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "SDPbaPd8j814",
    "outputId": "349ec585-c240-46f6-cafe-4439bf369b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 120000 training examples\n",
      "Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "print(f\"Dataset loaded: {len(dataset['train'])} training examples\")\n",
    "\n",
    "# View a sample\n",
    "sample = dataset['train'][0]\n",
    "\n",
    "print(f\"Text: {sample['text']}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgWMHByARCMv"
   },
   "source": [
    "## Prepare and Persist the Dataset\n",
    "\n",
    "Preprocess dataset to be in the instruction-tuning format of Alpaca dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740621169250,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "dzKl06R6aFqg",
    "outputId": "4a129127-cb3e-42ad-f266-0835e088fe6c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Colab Notebooks/CMU_LargeLanguageModels'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "610502de26604ca8b737b0940c2e34ae",
      "839b568b4fee4fa48da38e99924d8e4e",
      "e0d05c41d8df413ea79f746bcdf10d96",
      "86fd6fb19f634290be3fc3b92bbd3d77",
      "98a720741b5f4ec89d357bfdf6d3cd09",
      "01b77f3677724a1b82954112a91466d8",
      "c9fa2012e9864bea8099231e1e3c04aa",
      "d4d697ff16634143b46b28101abe939d",
      "cd476e2773584b34b3fd77d69e398373",
      "902a124ac8664e7eb76075ac0d9e3654",
      "734c6aa914e44618b9d5e7caa8edcbfd",
      "b0b6d4ec7c924af1b558178cd3d232da",
      "dd461c3ba3de4a9292c07ab48c065581",
      "45e0528837b6410aa5a65c288f2d1b92",
      "3911dc97c5ca4bdf83c94d171d88d8f8",
      "29a4589eceff456ea5ba9efae1632e2a",
      "bc2c5e7a39be467bafc9e94e7b7e4c1e",
      "d94f30701a50424fbedfc1656b784e1a",
      "0aa7aca5e7974089919651ec48d80a33",
      "3e8230dfae1c47678f6401532160de1e",
      "4f8e025a329748baa6fa5c804595048a",
      "d8eef20b2ad34536ad034c18ff878cee"
     ]
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1740621170088,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "Bcqejm_IQuZr",
    "outputId": "a886058f-8d50-4e7c-b157-d244d8f49fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business: 30000\n",
      "Sci/Tech: 30000\n",
      "Sports: 30000\n",
      "World: 30000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610502de26604ca8b737b0940c2e34ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b6d4ec7c924af1b558178cd3d232da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train Dataset features:  {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n",
      "Processed Test Dataset features:  {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n",
      "Processed Train Dataset: 120000 examples\n",
      "Processed Test Dataset: 7600 examples\n",
      "Sample Processed Train Dataset: {'instruction': 'Classify this news article.', 'input': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'output': 'Business'}\n",
      "Sample Processed Test Dataset: {'instruction': 'Classify this news article.', 'input': 'Group to Propose New High-Speed Wireless Format  LOS ANGELES (Reuters) - A group of technology companies  including Texas Instruments Inc. &lt;TXN.N&gt;, STMicroelectronics  &lt;STM.PA&gt; and Broadcom Corp. &lt;BRCM.O&gt;, on Thursday said they  will propose a new wireless networking standard up to 10 times  the speed of the current generation.', 'output': 'Sci/Tech'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Get label names for mapping\n",
    "label_names = dataset['train'].features['label'].names\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = Counter(dataset['train']['label'])\n",
    "\n",
    "# Print unique labels with counts\n",
    "for label_id, count in label_counts.items():\n",
    "    print(f\"{label_names[label_id]}: {count}\")\n",
    "\n",
    "# Prepare dataset with TorchTune format\n",
    "def preprocess_data(examples):\n",
    "    return {\n",
    "        \"instruction\": [\"Classify this news article.\"] * len(examples['text']),  # Repeating instruction\n",
    "        \"input\": examples['text'],\n",
    "        \"output\": [label_names[label] for label in examples['label']]\n",
    "    }\n",
    "\n",
    "processed_train = dataset[\"train\"].map(preprocess_data, batched=True)\n",
    "processed_test = dataset[\"test\"].map(preprocess_data, batched=True)\n",
    "\n",
    "processed_train = processed_train.remove_columns([\"text\", \"label\"])\n",
    "processed_test = processed_test.remove_columns([\"text\", \"label\"])\n",
    "\n",
    "# Save preprocessed dataset\n",
    "preprocessed_dataset = DatasetDict({\n",
    "    \"train\": processed_train,\n",
    "    \"test\": processed_test\n",
    "})\n",
    "\n",
    "# Persist preprocessed dataset\n",
    "save_path = \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/preprocessed_agnews\"\n",
    "preprocessed_dataset.save_to_disk(save_path)\n",
    "\n",
    "print(\"Processed Train Dataset features: \", processed_train.features)\n",
    "print(\"Processed Test Dataset features: \", processed_test.features)\n",
    "\n",
    "print(f\"Processed Train Dataset: {len(processed_train)} examples\")\n",
    "print(f\"Processed Test Dataset: {len(processed_test)} examples\")\n",
    "\n",
    "print(f\"Sample Processed Train Dataset: {processed_train[0]}\")\n",
    "print(f\"Sample Processed Test Dataset: {processed_test[10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2rj3KJv-zXn"
   },
   "source": [
    "## Load the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740621170092,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "XfAUJTS7aQDV",
    "outputId": "493fef1e-8498-4997-e435-488f6dc2add5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11823,
     "status": "ok",
     "timestamp": 1740621181916,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "q8NBq9UT-WGA",
    "outputId": "c578d366-ec7b-4244-ba32-857c69c7a284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed dataset with (train, test) samples: (120000 , 7600)\n",
      "Saved 120000 examples to JSONL file\n",
      "Saved 7600 examples to JSONL file\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "# Load preprocessed dataset from saved location\n",
    "dataset_path = \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/preprocessed_agnews\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(f\"Loaded preprocessed dataset with (train, test) samples: ({len(dataset['train'])} , {len(dataset['test'])})\")\n",
    "\n",
    "#dataset = dataset[\"train\"]\n",
    "#print(dataset[0])\n",
    "\n",
    "\n",
    "# Convert train to JSON Lines format\n",
    "train_data = dataset[\"train\"]\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/agnews_train.jsonl\", \"w\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(train_data)} examples to JSONL file\")\n",
    "\n",
    "test_data = dataset[\"test\"]\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/agnews_test.jsonl\", \"w\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(test_data)} examples to JSONL file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A-xmIAeWcsD"
   },
   "source": [
    "## FineTuning with LoRA using TorchTune\n",
    "\n",
    "**TorchTune** is a PyTorch library for LLM fine-tuning that prioritizes simplicity, correctness, and accessibility. It's designed to work seamlessly with PyTorch while making LLM experimentation accessible to everyone.\n",
    "\n",
    "**TorchTune Recipes**\n",
    "\n",
    "Recipes are the primary entry points for torchtune users. These can be thought of as hackable, singularly-focused scripts for interacting with LLMs including fine-tuning, inference, evaluation, and quantization.\n",
    "\n",
    "**LoRA** is a Parameter-efficient fine-tuning technique (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KreQ3GJrYoLE"
   },
   "source": [
    "### Full List of ALL Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10837,
     "status": "ok",
     "timestamp": 1740552102196,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "EoFemOLZYun3",
    "outputId": "9ff13b33-b134-4e5c-b14b-0784f694cc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgSOVDNwW9D2"
   },
   "source": [
    "### Customizing configs using tune cp\n",
    "\n",
    "There are 2 ways to customize recipe configs:\n",
    "\n",
    "* Using the tune cp command to copy a config from the torchtune library , modify it, and then use it when running the recipe.\n",
    "* Specifying the changed config values in a key=value format on the command line when running the recipe. Used this method for clarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1A9ADVScylL"
   },
   "source": [
    "### Get Model and Tokenizer for use with TorchTune CLI\n",
    "\n",
    "This is an alternate approach that uses TorchTune CLI over the use of the HuggingFace transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1740531600752,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "IZSWbB0fckio"
   },
   "outputs": [],
   "source": [
    "# This is not needed as the model and tokenizer are already present in the cache_dir\n",
    "#!tune download meta-llama/Llama-2-7b-hf --output-dir ./llama2-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XxwH3DRXshG"
   },
   "source": [
    "### Get a copy of Llama2 configs for TorchTune recipe\n",
    "\n",
    "Lookup corresponding Config for Recipe named -  lora_finetune_single_device, from tune ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8749,
     "status": "ok",
     "timestamp": 1740621190679,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "j72e4ALsAdJm",
    "outputId": "e4ccbb8a-1f06-4f5d-b9f2-c93ad5190043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device.yaml\n"
     ]
    }
   ],
   "source": [
    "# Copy llama2 config for recipe named lora_finetune_single_device\n",
    "# (recipe_name : config_names from tune ls)\n",
    "!tune cp llama2/7B_lora_single_device \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATZLxzi8X3Sv"
   },
   "source": [
    "### Make output directories for logging and model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740621190687,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "GaLsh_4bV319",
    "outputId": "3b27bb4a-b44a-4a95-9500-0bf0900041de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1740621190897,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "jhR5Iqu9WtY3"
   },
   "outputs": [],
   "source": [
    "!mkdir -p llama2_7B_lora_single_device_outputs\n",
    "!mkdir -p llama2_7B_lora_single_device_outputs/wandb_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlrKSvLmYMTQ"
   },
   "source": [
    "### FineTune with LoRA using TorchTune CLI\n",
    "\n",
    "Here we use the default configs and specify changes in the command line. Alternately, we could also make changes locally and specify path to the modified config.\n",
    "\n",
    "We train on 70% of the training data with a batch size of 2 and log to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740621190906,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "_OqYBMfj3L0o",
    "outputId": "421fd9e3-78b9-43df-db85-d740de851795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "a8e5f40590c7461bbee1a9bcb190a46d",
      "53157909a859442585df03ee36821d00",
      "d8a995fda3e04be18bf6a6771a1d19d8",
      "703f91023b8c456e973d9deeb6c6338f",
      "9037d7ef98784cf5af1c9155bea3431a",
      "415f7aeac7b142cb9ea088118b959e02",
      "da9258a225ad4be08b2f0ec38f03bd1c",
      "90a2cf2af0e24c84bf0eddb53a10433f",
      "539a72e9f6ad49dfabe7873abb621d80",
      "6c9f28113f5540eeb25724affbe078d2",
      "f38da82aab874617a848a803f3e6ca8d"
     ]
    },
    "executionInfo": {
     "elapsed": 2112,
     "status": "ok",
     "timestamp": 1740621193019,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "JxBC65otQkVP",
    "outputId": "ed226881-dcfb-41de-989d-a352e1556d44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e5f40590c7461bbee1a9bcb190a46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchtune.datasets import alpaca_cleaned_dataset\n",
    "\n",
    "# Define additional arguments for `load_dataset`\n",
    "load_dataset_kwargs = {\n",
    "    \"split\": \"train\",  # Load only 10% of the training data\n",
    "    \"data_files\": \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/agnews_train.jsonl\"\n",
    "}\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
    ")\n",
    "# Load the dataset\n",
    "dataset = alpaca_cleaned_dataset(source=\"json\", tokenizer=tokenizer, **load_dataset_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1740621193061,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "KXznZwNj8SCL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"<blah>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPCBRRKrZQZc",
    "outputId": "ca9aa7fd-7dc2-4d17-da02-8db8eeb0a168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  adapter_checkpoint: null\n",
      "  checkpoint_dir: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: LLAMA2\n",
      "  output_dir: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
      "  column_map:\n",
      "    input: input\n",
      "    instruction: instruction\n",
      "    output: output\n",
      "  data_files: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/agnews_train.jsonl\n",
      "  packed: false\n",
      "  source: json\n",
      "  split: train[:80%]\n",
      "  train_on_input: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 5\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  group: llama2_7b_lora_batch_8\n",
      "  job_type: lora_single_device\n",
      "  log_dir: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/wandb_logs\n",
      "  project: llama2_finetune_agnews_with_cli\n",
      "model:\n",
      "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 5\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1001341296. Local seed is seed + rank = 1001341296 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipti-aswath\u001b[0m (\u001b[33mdipti-aswath-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/wandb_logs/wandb/run-20250227_015319-ozt5fjkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrimson-butterfly-43\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dipti-aswath-carnegie-mellon-university/llama2_finetune_agnews_with_cli\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dipti-aswath-carnegie-mellon-university/llama2_finetune_agnews_with_cli/runs/ozt5fjkp\u001b[0m\n",
      "INFO:torchtune.utils._logging:Logging /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/torchtune_config.yaml to W&B under Files\n",
      "WARNING:torchtune.utils._logging:Error saving /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/torchtune_config.yaml to W&B.\n",
      "Error: \n",
      "[Errno 95] Operation not supported: '/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/torchtune_config.yaml' -> '/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/wandb_logs/wandb/run-20250227_015319-ozt5fjkp/files/torchtune_config.yaml'.Don't worry the config will be logged the W&B workspace\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 13.03 GiB\n",
      "\tGPU peak memory reserved: 13.04 GiB\n",
      "\tGPU peak memory active: 13.03 GiB\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|6000|Loss: 0.3096184730529785: 100% 6000/6000 [6:15:03<00:00,  3.77s/it] INFO:torchtune.utils._logging:Starting checkpoint save...\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device \\\n",
    "        --config llama2/7B_lora_single_device \\\n",
    "        output_dir=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs\" \\\n",
    "        checkpointer.checkpoint_dir=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/\" \\\n",
    "        tokenizer.path=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\" \\\n",
    "        dataset._component_=torchtune.datasets.alpaca_cleaned_dataset \\\n",
    "        dataset.source=\"json\" \\\n",
    "        dataset.column_map='{\"instruction\": \"instruction\", \"input\": \"input\", \"output\": \"output\"}' \\\n",
    "        dataset.data_files=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/agnews_train.jsonl\" \\\n",
    "        dataset.split=train[:80%] \\\n",
    "        dataset.train_on_input=False \\\n",
    "        lr_scheduler.num_warmup_steps=5 \\\n",
    "        batch_size=2 \\\n",
    "        gradient_accumulation_steps=8 \\\n",
    "        metric_logger._component_=torchtune.training.metric_logging.WandBLogger \\\n",
    "        metric_logger.project=llama2_finetune_agnews_with_cli  metric_logger.group=llama2_7b_lora_batch_8 \\\n",
    "        metric_logger.job_type=lora_single_device \\\n",
    "        metric_logger.log_dir=\"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/wandb_logs\" \\\n",
    "        log_every_n_steps=1 \\\n",
    "        log_peak_memory_stats=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfX5quHExqCB"
   },
   "source": [
    "### Observe fine tuned model v.s. base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1740643790501,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "viu67JLuQkR-",
    "outputId": "b5c15111-ac0c-44b2-c5bf-b2f1b8d45581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.9G\n",
      "-rw------- 1 root root  134 Feb 27 01:21 adapter_config.json\n",
      "-rw------- 1 root root  35M Feb 27 01:21 adapter_model.pt\n",
      "-rw------- 1 root root  35M Feb 27 01:21 adapter_model.safetensors\n",
      "-rw------- 1 root root  609 Feb 25 05:29 config.json\n",
      "-rw------- 1 root root 3.8G Feb 27 08:09 ft-model-00001-of-00002.safetensors\n",
      "-rw------- 1 root root  188 Feb 25 05:44 generation_config.json\n",
      "-rw------- 1 root root  28K Feb 27 01:21 model.safetensors.index.json\n",
      "-rw------- 1 root root  414 Feb 25 05:29 special_tokens_map.json\n",
      "-rw------- 1 root root  776 Feb 25 05:29 tokenizer_config.json\n",
      "-rw------- 1 root root 1.8M Feb 25 05:29 tokenizer.json\n",
      "-rw------- 1 root root 489K Feb 25 05:29 tokenizer.model\n",
      "-rw------- 1 root root 2.6K Feb 26 17:32 torchtune_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Fine tuned model and LoRA adapter checkpoints\n",
    "%ls -lh \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/llama2_7B_lora_single_device_outputs/epoch_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1740643790717,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "EGPEULKVra_4",
    "outputId": "b76feb30-afc1-40c6-e0d9-d838769f5b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9.0K\n",
      "lrw------- 1 root root   52 Feb 25 05:29 \u001b[0m\u001b[01;36mconfig.json\u001b[0m -> ../../blobs/34f901200fa131819b355bc4bed876c957a77a5a\n",
      "lrw------- 1 root root   52 Feb 25 05:44 \u001b[01;36mgeneration_config.json\u001b[0m -> ../../blobs/aa1b3d3486df56a0699ce90c33283b13556fb5a3\n",
      "lrw------- 1 root root   76 Feb 25 05:30 \u001b[01;36mmodel-00001-of-00002.safetensors\u001b[0m -> ../../blobs/4ec71fd53e99766de38f24753b30c9e8942630e9e576a1ba27b0ec531e87be41\n",
      "lrw------- 1 root root   76 Feb 25 05:30 \u001b[01;36mmodel-00002-of-00002.safetensors\u001b[0m -> ../../blobs/41780b5dac322ac35598737e99208d90bdc632a1ba3389ebedbb46a1d8385a7f\n",
      "lrw------- 1 root root   52 Feb 25 05:29 \u001b[01;36mmodel.safetensors.index.json\u001b[0m -> ../../blobs/8b6245796e966e50960a317e4a54aa7bf73b0186\n",
      "lrw------- 1 root root   76 Feb 25 06:16 \u001b[01;36mpytorch_model-00001-of-00002.bin\u001b[0m -> ../../blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
      "lrw------- 1 root root   76 Feb 25 06:17 \u001b[01;36mpytorch_model-00002-of-00002.bin\u001b[0m -> ../../blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
      "lrw------- 1 root root   52 Feb 25 06:16 \u001b[01;36mpytorch_model.bin.index.json\u001b[0m -> ../../blobs/db7264b24cac7a39947bb5fc02fe5c2d7ac9eaf4\n",
      "lrw------- 1 root root   52 Feb 25 05:29 \u001b[01;36mspecial_tokens_map.json\u001b[0m -> ../../blobs/451134b2ddc2e78555d1e857518c54b4bdc2e87d\n",
      "lrw------- 1 root root   52 Feb 25 05:29 \u001b[01;36mtokenizer_config.json\u001b[0m -> ../../blobs/2ef41cbc275000b29afe157ba487f0530b8c26dc\n",
      "lrw------- 1 root root   52 Feb 25 05:29 \u001b[01;36mtokenizer.json\u001b[0m -> ../../blobs/a6e931b92caff4c79c5c56282f1e89569a0ae558\n",
      "lrw------- 1 root root   76 Feb 25 05:29 \u001b[01;36mtokenizer.model\u001b[0m -> ../../blobs/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
      "-rw------- 1 root root 2.6K Feb 27 01:53 torchtune_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Base Model without fine tuning\n",
    "%ls -lh \"/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApzINjJ6EBV"
   },
   "source": [
    "## Run Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ehJdWWxpXsL"
   },
   "source": [
    "#### Inference with Baseline Model\n",
    "\n",
    "See *model_inference.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1740719137620,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "H291NSDGAkBU",
    "outputId": "d5787939-f8b0-442e-b7be-631c144b641f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lOKkRKXprj0"
   },
   "source": [
    "### Inference with the Fine Tuned Model using PEFT from [HuggingFace library](https://github.com/huggingface/peft)\n",
    "\n",
    "See *model_inference_py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHVz2XxX64hu",
    "outputId": "5250e742-410c-48e0-af9e-b9d6b1aabb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 05:06:08.533072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740719168.834962    3338 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740719168.914968    3338 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 05:06:09.540321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading baseline model: meta-llama/Llama-2-7b-hf from cache_path: /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/cache\n",
      "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python model_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDKJxLCxsiZm"
   },
   "source": [
    "## Evaluating the fine-tuned LLM\n",
    "\n",
    "Fine-tuned LLMs can be evaluated through multiple approaches:\n",
    "* With Short-answer and multiple-choice benchmarks such as Measuring Massive Mutitask Language Understanding (MMLU to test the general knowledge of a model)\n",
    "* With Human preference comparison to other LLMs\n",
    "* With Automated conversational benchmarks, where another LLM is used to evaluate the responses.\n",
    "\n",
    "Here, we are primarily interested in assessing conversational performance rather than just the ability to answer multiple-choice questions, and so human evaluation and automated metrics are more relevant.\n",
    "\n",
    "We use the method here to automate the response evaluation of the fine-tuned Llama2 using another, larger LLM - DeepSeek-R1-Distill-Llama-8B.\n",
    "\n",
    "We will use Ollama to run this model locally as shown below.\n",
    "\n",
    "**DeepSeek-R1-Distill-Llama-8B**\n",
    "\n",
    "`ollama run deepseek-r1:8b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1740686133861,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "B4JklMnOykKn",
    "outputId": "c8878d6e-0eff-48b0-b2a5-0fe5d61fdb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "!ollama run deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740687366807,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "L_044mvI-0gq",
    "outputId": "81f696da-fac3-4399-e2a8-60033a0a0e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks/CMU_LargeLanguageModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1740687368112,
     "user": {
      "displayName": "Dipti Aswath",
      "userId": "02579527532735105708"
     },
     "user_tz": 480
    },
    "id": "VIRIkacJyql3",
    "outputId": "78244558-26b5-4da1-8c2f-5316ea359ff8"
   },
   "outputs": [],
   "source": [
    "!python ollama_evaluate.py --file_path=\"./agnews_test_response.jsonl\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhVzmHka9zeIMX7o9AXgjK",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b77f3677724a1b82954112a91466d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0aa7aca5e7974089919651ec48d80a33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f17447703644549818025c2036a8ec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3da7bd85ef8a4ca19da51c792e5bb4ac",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a1892fbd1c64e6c9d7bc4d64d88dbf3",
      "value": 2
     }
    },
    "29a4589eceff456ea5ba9efae1632e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3911dc97c5ca4bdf83c94d171d88d8f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f8e025a329748baa6fa5c804595048a",
      "placeholder": "​",
      "style": "IPY_MODEL_d8eef20b2ad34536ad034c18ff878cee",
      "value": " 7600/7600 [00:00&lt;00:00, 231828.94 examples/s]"
     }
    },
    "3da7bd85ef8a4ca19da51c792e5bb4ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e8230dfae1c47678f6401532160de1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "415f7aeac7b142cb9ea088118b959e02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45e0528837b6410aa5a65c288f2d1b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0aa7aca5e7974089919651ec48d80a33",
      "max": 7600,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e8230dfae1c47678f6401532160de1e",
      "value": 7600
     }
    },
    "47bdc671486743fbaec75be63e5a0fc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8e025a329748baa6fa5c804595048a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5054142b3b01464e8c896aad70cc8e99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82322fade0044806a93a4898057b9526",
      "placeholder": "​",
      "style": "IPY_MODEL_e367d720ee9947cf89863d95e2fa3643",
      "value": " 2/2 [03:52&lt;00:00, 107.38s/it]"
     }
    },
    "53157909a859442585df03ee36821d00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_415f7aeac7b142cb9ea088118b959e02",
      "placeholder": "​",
      "style": "IPY_MODEL_da9258a225ad4be08b2f0ec38f03bd1c",
      "value": "Generating train split: "
     }
    },
    "539a72e9f6ad49dfabe7873abb621d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "610502de26604ca8b737b0940c2e34ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_839b568b4fee4fa48da38e99924d8e4e",
       "IPY_MODEL_e0d05c41d8df413ea79f746bcdf10d96",
       "IPY_MODEL_86fd6fb19f634290be3fc3b92bbd3d77"
      ],
      "layout": "IPY_MODEL_98a720741b5f4ec89d357bfdf6d3cd09"
     }
    },
    "6a1892fbd1c64e6c9d7bc4d64d88dbf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c9f28113f5540eeb25724affbe078d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "703f91023b8c456e973d9deeb6c6338f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c9f28113f5540eeb25724affbe078d2",
      "placeholder": "​",
      "style": "IPY_MODEL_f38da82aab874617a848a803f3e6ca8d",
      "value": " 120000/0 [00:00&lt;00:00, 623236.33 examples/s]"
     }
    },
    "734c6aa914e44618b9d5e7caa8edcbfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82322fade0044806a93a4898057b9526": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "839b568b4fee4fa48da38e99924d8e4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01b77f3677724a1b82954112a91466d8",
      "placeholder": "​",
      "style": "IPY_MODEL_c9fa2012e9864bea8099231e1e3c04aa",
      "value": "Saving the dataset (1/1 shards): 100%"
     }
    },
    "84b0c8e5b88c42d69eead53736f12c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad8bbe2c1aee46b8bc56503252b05740",
      "placeholder": "​",
      "style": "IPY_MODEL_a96cf73088ba4b5fbe340711a463c4bb",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "86fd6fb19f634290be3fc3b92bbd3d77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902a124ac8664e7eb76075ac0d9e3654",
      "placeholder": "​",
      "style": "IPY_MODEL_734c6aa914e44618b9d5e7caa8edcbfd",
      "value": " 120000/120000 [00:00&lt;00:00, 519297.51 examples/s]"
     }
    },
    "902a124ac8664e7eb76075ac0d9e3654": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9037d7ef98784cf5af1c9155bea3431a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90a2cf2af0e24c84bf0eddb53a10433f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "98a720741b5f4ec89d357bfdf6d3cd09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8e5f40590c7461bbee1a9bcb190a46d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53157909a859442585df03ee36821d00",
       "IPY_MODEL_d8a995fda3e04be18bf6a6771a1d19d8",
       "IPY_MODEL_703f91023b8c456e973d9deeb6c6338f"
      ],
      "layout": "IPY_MODEL_9037d7ef98784cf5af1c9155bea3431a"
     }
    },
    "a96cf73088ba4b5fbe340711a463c4bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad8bbe2c1aee46b8bc56503252b05740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0b6d4ec7c924af1b558178cd3d232da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd461c3ba3de4a9292c07ab48c065581",
       "IPY_MODEL_45e0528837b6410aa5a65c288f2d1b92",
       "IPY_MODEL_3911dc97c5ca4bdf83c94d171d88d8f8"
      ],
      "layout": "IPY_MODEL_29a4589eceff456ea5ba9efae1632e2a"
     }
    },
    "bc2c5e7a39be467bafc9e94e7b7e4c1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c61ad0975df24ee29464a1cbc23284aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_84b0c8e5b88c42d69eead53736f12c9e",
       "IPY_MODEL_0f17447703644549818025c2036a8ec3",
       "IPY_MODEL_5054142b3b01464e8c896aad70cc8e99"
      ],
      "layout": "IPY_MODEL_47bdc671486743fbaec75be63e5a0fc5"
     }
    },
    "c9fa2012e9864bea8099231e1e3c04aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd476e2773584b34b3fd77d69e398373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4d697ff16634143b46b28101abe939d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8a995fda3e04be18bf6a6771a1d19d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90a2cf2af0e24c84bf0eddb53a10433f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_539a72e9f6ad49dfabe7873abb621d80",
      "value": 1
     }
    },
    "d8eef20b2ad34536ad034c18ff878cee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d94f30701a50424fbedfc1656b784e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da9258a225ad4be08b2f0ec38f03bd1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd461c3ba3de4a9292c07ab48c065581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc2c5e7a39be467bafc9e94e7b7e4c1e",
      "placeholder": "​",
      "style": "IPY_MODEL_d94f30701a50424fbedfc1656b784e1a",
      "value": "Saving the dataset (1/1 shards): 100%"
     }
    },
    "e0d05c41d8df413ea79f746bcdf10d96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4d697ff16634143b46b28101abe939d",
      "max": 120000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd476e2773584b34b3fd77d69e398373",
      "value": 120000
     }
    },
    "e367d720ee9947cf89863d95e2fa3643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f38da82aab874617a848a803f3e6ca8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
